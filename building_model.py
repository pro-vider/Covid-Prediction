# -*- coding: utf-8 -*-
"""Building Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CsHPxozuf2rlzbfJ6laR11rJiL7NcAHe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("Covid Dataset.csv")

data.drop_duplicates(inplace=True)
data.reset_index(drop=True, inplace=True)

from sklearn.preprocessing import LabelEncoder
e=LabelEncoder()

data['Breathing Problem']=e.fit_transform(data['Breathing Problem'])
data['Fever']=e.fit_transform(data['Fever'])
data['Dry Cough']=e.fit_transform(data['Dry Cough'])
data['Sore throat']=e.fit_transform(data['Sore throat'])
data['Running Nose']=e.fit_transform(data['Running Nose'])
data['Asthma']=e.fit_transform(data['Asthma'])
data['Chronic Lung Disease']=e.fit_transform(data['Chronic Lung Disease'])
data['Headache']=e.fit_transform(data['Headache'])
data['Heart Disease']=e.fit_transform(data['Heart Disease'])
data['Diabetes']=e.fit_transform(data['Diabetes'])
data['Hyper Tension']=e.fit_transform(data['Hyper Tension'])
data['Abroad travel']=e.fit_transform(data['Abroad travel'])
data['Contact with COVID Patient']=e.fit_transform(data['Contact with COVID Patient'])
data['Attended Large Gathering']=e.fit_transform(data['Attended Large Gathering'])
data['Visited Public Exposed Places']=e.fit_transform(data['Visited Public Exposed Places'])
data['Family working in Public Exposed Places']=e.fit_transform(data['Family working in Public Exposed Places'])
data['Wearing Masks']=e.fit_transform(data['Wearing Masks'])
data['Sanitization from Market']=e.fit_transform(data['Sanitization from Market'])
data['COVID-19']=e.fit_transform(data['COVID-19'])
data['Gastrointestinal ']=e.fit_transform(data['Gastrointestinal '])
data['Fatigue ']=e.fit_transform(data['Fatigue '])

data.head()

data.pop('Wearing Masks')
data.pop('Sanitization from Market')
data.pop('Abroad travel')
y=data.pop('COVID-19')
x=data

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)

from collections import Counter
from imblearn.over_sampling import RandomOverSampler
print("Before oversampling: ", Counter(y_train))
oversample = RandomOverSampler(sampling_strategy='minority')
x_train_over, y_train_over = oversample.fit_resample(x_train, y_train)
print("After oversampling: ", Counter(y_train_over))

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

key = ['LogisticRegression','SVC','DecisionTreeClassifier','RandomForestClassifier','KNeighborsClassifier','GaussianNB']
value = [LogisticRegression(),SVC(),DecisionTreeClassifier(),RandomForestClassifier(),KNeighborsClassifier(),GaussianNB()]
models = dict(zip(key,value))
print(models)

predicted =[]
from sklearn.metrics import classification_report
for name,algo in models.items():
    model=algo
    model.fit(x_train_over,y_train_over)
    predict = model.predict(x_test)
    acc = accuracy_score(y_test, predict)
    predicted.append(acc)
    print(name,acc)
    print(confusion_matrix(y_test,predict))
    print(classification_report(y_test, predict))

sns.barplot(x=predicted, y=key)

"""Now we will build another model based on the features that showed major impact in our data analysis.

"""

x=data[['Breathing Problem','Fever','Dry Cough','Sore throat']]
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)
#,'Abroad travel'

x.head()

print("Before oversampling: ", Counter(y_train))
oversample = RandomOverSampler(sampling_strategy='minority')
x_train_over, y_train_over = oversample.fit_resample(x_train, y_train)
print("After oversampling: ", Counter(y_train_over))

predicted =[]
from sklearn.metrics import classification_report
for name,algo in models.items():
    model=algo
    model.fit(x_train_over,y_train_over)
    predict = model.predict(x_test)
    acc = accuracy_score(y_test, predict)
    predicted.append(acc)
    print(name,acc)
    print(confusion_matrix(y_test,predict))
    print(classification_report(y_test, predict))

sns.barplot(x=predicted, y=key)

"""Since with only 5 features the accuracy is same for both the models. Specially SVC/ Logistic Regression/ Random Forest models has similar accuracy it makes more sense to use only the selected fieds in order to keep the model simple and cost/memory effective.

Among the 3 best models we will go ahead with Random Forest.

We will also try out Hyperparameter Tuning on Random Forest model to see if that improves Model accuracy.
"""

from sklearn.model_selection import GridSearchCV

k_range = list(range(1, 31))
param_grid = dict(n_neighbors=k_range)
# Create a based model
knn = KNeighborsClassifier()
# Instantiate the grid search model
grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy',
                    return_train_score=False,verbose=1)

grid.fit(x_train_over,y_train_over)

print(grid.best_estimator_)

grid_predictions = grid.predict(x_test)
acc = accuracy_score(y_test, grid_predictions)
print(acc)
print(confusion_matrix(y_test,grid_predictions))
print(classification_report(y_test,grid_predictions))

"""Hyperparameter Tuning has not increased the accuracy.

We will go ahead and save the the model.

"""

import pickle
from joblib import Parallel, delayed
import joblib
# Save the trained model as a pickle string.

joblib.dump(grid, 'Covid_model.pkl')

from google.colab import drive
drive.mount('/content/drive')

model=joblib.load('Covid_model.pkl')
preds=model.predict(x_test)

acc = accuracy_score(y_test, preds)
print(acc)
print(confusion_matrix(y_test,preds))
print(classification_report(y_test,preds))

